
import java.io.*;
import java.nio.charset.Charset;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.util.*;

/**
 * Created by Petter Glad-Ã˜rbak.
 */
public class TagPrediction
{
    // Path of image folder to classify.
    public static String IMG_PATH = "D:/img2/";

    // Path to the word2vec model.
    public static final String VECTOR_MODEL_PATH = "D:/Master/w2v/models/GoogleNews-vectors-negative300-SLIM.bin";
    public static final String VECTOR_MODEL_PATH2 = "D:/Master/w2v/models/Flickr25k.zip";
    public static final String VECTOR_MODEL_PATH3 = "D:/Master/w2v/models/Flickr368k.zip";
    public static final String VECTOR_MODEL_PATH4 = "D:/Master/NewExperiment.zip";

    public static final String TAG_FILES_PATH = "D:/tags/tags/";


    public static BufferedWriter BW;

    public static void main(String[] args)
    {
        try {

            /*
            For a folder of tag text files, tags for each image in separate file;
            Read all files, write them to a single file with the tags of each image per line.
             */
            conjoinTagFiles(TAG_FILES_PATH);

            /*
            Classifies all images in folder IMG_PATH, and writes them to file.
            Each line in the output file corresponds to the predictions for an image.
             */
            ImageClassifier ic = new ImageClassifier(IMG_PATH);


            System.out.println("Loading vector model..");
            double time1 = System.currentTimeMillis();

            /*
            Loads a pre-trained Word2vec model from VECTOR_MODEL_PATH.
             */
            WordVectorUtil wvu = new WordVectorUtil(VECTOR_MODEL_PATH3);
            double time2 = System.currentTimeMillis();
            System.out.println("Took " + (time2-time1)/1000 + " seconds to load vector model.");


            System.out.println("Merging predictions with their nearest neighbors and writing to file..");
            double time3 = System.currentTimeMillis();

            /*
            For each line in the text output file generated by the classifier;
            Find the N vectors of closest cosine similarity and merge them with the predictions.
            Write them to a new file. Each line represents predictions and their N semantic neighbors
            for an image.
             */
            mergePredictionsAndVectorNeighbors(wvu, 5);
            double time4 = System.currentTimeMillis();
            System.out.println("Took " + (time4-time3)/1000 + " seconds to merge and write.");
            System.out.println("Calculating number of predicted tags..");
            double time5 = System.currentTimeMillis();


            /*
            From the text file of predictions and semantic neighbors;
            For each line, compare matches in same line in conjoined text file of tags.
            Order of image files and tag files are identical.
            After classification and conjoining of tag files, each line in both
            files corresponds to the same image.

            Compute performance measures and print results.
             */
            calculateTagPrediction();
            double time6 = System.currentTimeMillis();
            System.out.println("Took " + (time6-time5)/1000 + " seconds to calculate predicted tags.");
            System.out.println("Total program time: " + (time6-time5)/1000 + " seconds.");


        } catch (Exception e)
        {
            System.out.println("Error occured: " + e);
            e.printStackTrace();
        }




    }
    /*
    Get nearest 'numberOfNearest' neighbors of each prediction from the word2vec model,
    then write them to file. New lines then consist of the predictions and their 'numberOfNearest'
    neighbors according to the word2vec model.
     */
    public static void mergePredictionsAndVectorNeighbors(WordVectorUtil wvu, int numberOfNearest)
    {
        try {
            File predictionsText = new File("D:/Master/top1predictions.txt/");
            BW = new BufferedWriter(new FileWriter("D:/Master/gen/top1-5-Flickr368.txt"));


            List<String> predictions = readAllLinesOrExit(predictionsText.toPath());
            List<String> predictionsAndNeighbors = new ArrayList<>();

            for(String s : predictions)
            {

                String newLine = "";

                for(String word : s.split(" "))
                {
                    double time1=System.currentTimeMillis();

                    if(word.length() > 1);
                    {
                        String neighborString = wvu.getNearestWordsAsString(word, numberOfNearest);

                        double time2 = System.currentTimeMillis();
                        System.out.println((time2 - time1) / 1000);
                        if (!neighborString.isEmpty()) {
                            newLine += neighborString;
                        }
                        double time3 = System.currentTimeMillis();
                        System.out.println((time3 - time2) / 1000);
                    }
                }
                Set<String> sett = new LinkedHashSet<>();
                String[] removeDuplicatesArray = (s+newLine).split(" ");
                for(String n : removeDuplicatesArray)
                {
                    sett.add(n.toLowerCase());
                }
                String withoutDuplicates = "";
                for(String m : sett)
                {
                    if(m.length() > 1);
                    {
                        withoutDuplicates += m + " ";
                    }
                }
                predictionsAndNeighbors.add(withoutDuplicates);
            }
            for(String s : predictionsAndNeighbors)
            {
                BW.write(s);
                BW.newLine();
            }
            BW.close();


        } catch (Exception e)
        {
            System.out.println("Error occured: " + e);
            e.printStackTrace();
        }
    }
    /*
    Check each of the predictions and their neighbors against the tags.
    Keep track of tested words to avoid duplicate matches.
    Compute average for corpus (subtract for number of images without tags).
     */
    public static void calculateTagPrediction()
    {
        try
        {
            File predsFile = new File("D:/Master/gen/top5-2-Flickr368.txt");
            File tagsFile = new File("D:/Master/tagsfile-1000-1999.txt");

            List<String> predsList = readAllLinesOrExit(predsFile.toPath());
            List<String> tagsList = readAllLinesOrExit(tagsFile.toPath());


            double totalPrecision = 0.0;
            double totalRecall = 0.0;

            int nonZero = 0;
            int empty = 0;
            int totalMatches = 0;
            int totalGuesses = 0;
            int exper = 0;


            for(String s : tagsList)
            {
                if(s.equals("") || s.isEmpty() || s.equals(" ")) empty++;
            }

            System.out.println("Number of empty: " + empty);

            System.out.println("Starting matching..");

            for(String s : predsList)
            {
                int numberOfGuesses = s.split(" ").length;
                int numberOfTags = 0;
                int counter = 0;
                int predsListIndex = predsList.indexOf(s);

                List<String> testedWords = new ArrayList<>();


                for(String k : s.split(" "))
                {
                        totalGuesses++;
                        String[] sarr = tagsList.get(predsListIndex).toLowerCase().split(" ");
                        List<String> tagsForImage = new ArrayList<>();



                        for(String u : sarr)
                        {
                            tagsForImage.add(u);
                        }

                    if(tagsForImage.size() != 0)
                    {
                        numberOfTags = tagsForImage.size();
                    }

                        if(tagsForImage.contains(k.toLowerCase()) && !testedWords.contains(k.toLowerCase()))
                        {
                            counter++;
                            System.out.println("Match found: " + k);
                            testedWords.add(k);
                        }

                }
                totalMatches += counter;

                if(counter > 0)
                {
                    nonZero++;
                    double precisionForThisImage = (double)counter/numberOfGuesses;

                    if(predsListIndex > 100)
                        System.out.println("im1" + predsListIndex + ".jpg");
                    else {
                        System.out.println("im10" + predsListIndex + ".jpg");
                    }
                    System.out.println("Number of matches for string [" + s + "] among [" + tagsList.get(predsListIndex) + "]: " + counter);
                    System.out.println("Precision for image: " + precisionForThisImage);
                    if(numberOfTags != 0)
                    {
                        double recallForThisImage = (double)counter / numberOfTags;
                        totalRecall += recallForThisImage;
                        System.out.println("Recall for image: " + recallForThisImage);
                    }

                    totalPrecision += precisionForThisImage;

                }

            }
            double average = (double)totalMatches/(predsList.size()-empty);
            double averagePrecision = (totalPrecision/(predsList.size()-empty))*100;
            double averageRecall = (totalRecall/(predsList.size()-empty))*100;
            double fscore = (2*((averagePrecision*averageRecall)/(averagePrecision+averageRecall)));

            double trueAvg = (double)nonZero/(predsList.size()-empty);

            System.out.println("Total matches found in " + predsList.size() + " images was: " + totalMatches);
            System.out.println("Average was: " + average);
            System.out.println("Number of guesses: " + totalGuesses);
            System.out.println("Average precision: " + averagePrecision);
            System.out.println("Average recall: " + averageRecall);
            System.out.println("F-score: " + fscore);
            System.out.println("Non-zero recall: " + nonZero);
            System.out.println("True average accuracy: " + trueAvg);



        } catch (Exception e)
        {
            System.out.println("Error occured: " + e);
            e.printStackTrace();
        }


    }
    /*
    Read the tags from each file, convert to lower case and place them on one line in a new file.a
     */
    public static void conjoinTagFiles(String path)
    {
        try {
            BufferedWriter writer = new BufferedWriter(new FileWriter("tagsfile-1000-1999.txt"));
            List<Path> tagPathsList = getPaths(path);
            for (Path p : tagPathsList)
            {
                List<String> fileContent = readAllLinesOrExit(p);

                for(String s : fileContent)
                {
                    writer.write(s.toLowerCase() + " ");
                }
                writer.newLine();
            }
            writer.close();


        } catch (Exception e)
        {
            System.out.println("Error occured: " + e);
            e.printStackTrace();
        }
    }
    /*
    Return a list of Path objects to all the files in a given folder.
     */
    private static List<Path> getPaths(String path)
    {
        File[] files = new File(path).listFiles();

        List<Path> pathList = new ArrayList<>();

        for (File file : files) {
            if (file.isFile()) {
                pathList.add(Paths.get(file.getAbsolutePath()));
            }
        }
        return pathList;
    }
    /*
    Return a text file as a List<String> where each line of text is an object.
     */
    private static List<String> readAllLinesOrExit(Path path) {
        try {
            return Files.readAllLines(path, Charset.forName("UTF-8"));
        } catch (IOException e) {
            System.err.println("Failed to read [" + path + "]: " + e.getMessage());
            System.exit(0);
        }
        return null;
    }
}
